---
title: "MALA and Ising"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE, dev = "png", dpi = 300)
```


Below is the function that implements a simple random walk Metropolis algorithm with the given proposals to target a standard multivariate normal distribution of dimension $p$.

```{r packages}
library(MASS)
library(ggplot2)
library(dplyr)
library(coda)

set.seed(42)
```

```{r rw m-h}
RWMetropolisHastings <- function(n=1000, p, l, Sigma=(l^2/p) * diag(1, nrow=p), 
                               x0=numeric(p)){
  count <- 0
  X <- matrix(0, nrow=p, ncol=n)
  X[, 1] <- x0
  U <- runif(n)
  sum_sq_jumps <- 0
  
  for (i in 2:n) {
    Y <- X[,i-1] + mvrnorm(mu=numeric(p), Sigma=Sigma)
    
    alpha <- exp(-0.5 * sum(Y^2)) / exp(-0.5 * sum(X[, i-1]^2))
      
    if (U[i] < alpha) {
      X[, i] <- Y
      count <- count + 1
      sum_sq_jumps <- sum_sq_jumps + sum((Y-X[, i-1])^2)
    } else {
      X[, i] <- X[, i-1]
    }
  }
  msjd <- sum_sq_jumps / (n-1)
  
  return(list(X=X, accepted=count, acceptance_rate = count/(n-1), msjd=msjd))
}
```

```{r, optimal scaling for p and l}
p.vals <- c(1, 5, 20, 50, 100)
l.vals <- seq(0.1, 5, length.out=30)

results <- data.frame()

for (p in p.vals) {
  for (l in l.vals) {
    res <- RWMetropolisHastings(n=5000, p=p, l=l)
    results <- rbind(results, data.frame(
      p=p,
      l=l,
      acc.rate=res$acceptance_rate,
      msjd=res$msjd
    ))
  }
}
```

```{r, plot of l against msjd, fig.width=8, fig.height=5}
ggplot(results, aes(x=l, y=msjd, color=as.factor(p))) +
  geom_line() +
  geom_vline(xintercept = 2.38, linetype="dashed") +
  labs(title = "RWMH: MSJD vs Scaling Parameter l",
       subtitle = "Dashed line represents theoretical optimum l = 2.38",
       x = "Scaling (l)", y = "MSJD", color = "Dimension (p)") +
  theme_minimal()
```

```{r, plot of l against acc.rate, fig.width=8, fig.height=5}
ggplot(results, aes(x=l, y=acc.rate, color=as.factor(p))) +
  geom_line() +
  geom_hline(yintercept = 0.234, linetype="dashed") + 
  labs(title = " RWMH: Acceptance Rate vs Scaling Parameter l",
       subtitle = "Dashed line represents theoretical optimum rate = 0.234",
       x = "Scaling (l)", y = "Acceptance Rate", color= "Dimension (p)") +
  theme_minimal()
```
The first graph illustrates the efficiency of the chain via the MSJD. When $l$ is low, the MSJD is also low because the chain moves too slowly. When $l$ is large the MSJD is low because the chain gets 'stuck' due to high rejection rates. The optimum of $l=2.38$ maximises the efficiency between acceptance rate and MSJD. We can see that the MSJD is maximised around $l=2.38$ in dimensions of $p \ge 5$ confirming that setting the scaling parameter near this value balances the step size against the rejection probability to maximise the mixing of the chain. The second graph shows that as we increase $l$ we decrease the acceptance rate. This makes sense intuitively because we are taking larger proposed jumps which means we have an increased probability of proposing states that are in the tails leading to frequent rejections.


The acceptance probability for the MALA algorithm is given by
$$
\alpha(Y|X) = \min \left(1, \frac{f(Y)}{f(X)}\frac{q(X|Y)}{q(Y|X)}\right)
$$
Here we can derive each term separately and then substitute them into the definition above. For this problem we are targeting a standard multivariate normal distribution so 
$$
f(\textbf{y}) = (2\pi)^{-p/2} \exp\left(-\frac 12 \sum_{i=1}^{p}y_i^{2}\right)
$$
since the determinant of $\mathbb{I}_p$ is 1 and the inverse of the identity matrix is also the identity matrix. By a similar argument,
$$
f(\textbf{x}) = (2\pi)^{-p/2} \exp\left(-\frac 12 \sum_{i=1}^{p}x_i^{2}\right)
$$
Hence,
$$
\frac{f(\textbf{y})}{f(\textbf{x})} = \exp\left(-\frac 12 \sum_{j=1}^{p}[y_j^2 - x_j^2]\right)
$$
For the proposal, we know that
$$
\log f(\textbf{x}) = -\frac{p}{2}\log (2\pi) - \frac 12\sum_{i=1}^{p}x_i^2
$$
Then,
$$
\nabla \log f(\textbf{x}) = -\textbf{x}
$$
Now, using the definition of the proposal $q(y|x)$ with the above result we have that, after some algebraic manipulation we get
$$
q(\textbf{y}|\textbf{x}) = (2\pi)^{-p/2}\left(\frac{l^2}{p^{1/3}}\right)^p \exp \left(-\frac{p^{1/3}}{2l^2}\left(\textbf{y}-\textbf{x}+\frac{l^2}{2p^{1/3}}\textbf{x}\right)^T\left(\textbf{y}-\textbf{x}+\frac{l^2}{2p^{1/3}}\textbf{x}\right)\right)
$$
Similarly, we find that
$$
q(\textbf{x}|\textbf{y}) = (2\pi)^{-p/2}\left(\frac{l^2}{p^{1/3}}\right)^p \exp \left(-\frac{p^{1/3}}{2l^2}\left(\textbf{x}-\textbf{y}+\frac{l^2}{2p^{1/3}}\textbf{y}\right)^T\left(\textbf{x}-\textbf{y}+\frac{l^2}{2p^{1/3}}\textbf{y}\right)\right)
$$
Define $\sigma_p^2=\frac{l^2}{2p^{1/3}}$ we see that
$$
\frac{q(\textbf{x}|\textbf{y})}{q(\textbf{y}|\textbf{x})} = \frac{\exp \left(-\frac{1}{2\sigma_p^2}\left(\textbf{x}-\textbf{y}\left(1-\frac{\sigma_p^2}{2}\right)\right)^T\left(\textbf{x} - \textbf{y}\left(1-\frac{\sigma^2_p}{2}\right)\right)\right)}{\exp \left(-\frac{1}{2\sigma_p^2}\left(\textbf{y}-\textbf{x}\left(1-\frac{\sigma_p^2}{2}\right)\right)^T\left(\textbf{y} - \textbf{x}\left(1-\frac{\sigma^2_p}{2}\right)\right)\right)}
$$
This then simplifies to the following where we consider the summation over the components of the vectors $\textbf{x}$ and $\textbf{y}$,
$$
\frac{q(\textbf{x}|\textbf{y})}{q(\textbf{y}|\textbf{x})} = \exp \left[-\frac{1}{2\sigma_p^2}\sum_{j=1}^{p}\left(x_j-\left(1-\frac{\sigma^2_p}{2}\right)y_j\right)^2 - \left(y_j-\left(1-\frac{\sigma^2_p}{2}\right)x_j\right)^2\right]
$$
Hence we get the final $\log \alpha(\textbf{y}|\textbf{x})$ as,
$$
\log \alpha(\textbf{y}|\textbf{x}) = \\
\min \left\{0, -\frac 12 \sum_{j=1}^{p}[y_j^2 - x_j^2] -\frac{1}{2\sigma_p^2}\sum_{j=1}^{p}\left(x_j-\left(1-\frac{\sigma^2_p}{2}\right)y_j\right)^2 - \left(y_j-\left(1-\frac{\sigma^2_p}{2}\right)x_j\right)^2\right\}
$$


Below is the implementation of the MALA. Here we consider different values for the dimension $p$ i.e., $p=1$ and $p=5$ and the scaling constant $l$.

```{r MALA}
MALA <- function(n=1000, p, l, Sigma=(l^(2)/p^(1/3)) * diag(1, nrow=p), 
                               x0=numeric(p)){
  count <- 0
  X <- matrix(0, nrow=p, ncol=n)
  X[, 1] <- x0
  U <- runif(n)
  sum_sq_jumps <- 0
  
  for (i in 2:n) {
    Y <- X[,i-1] + l^(2)/(2*p^(1/3)) * (-X[,i-1]) + mvrnorm(mu=numeric(p),
                                                            Sigma=Sigma)
    
    sigma.sqd = diag(Sigma)[1]
    
    lg.alpha <- -(1/2) * sum(Y^2 - X[, i-1]^2) -
    (1/(2*sigma.sqd)) * (sum((X[,i-1] - (1 - sigma.sqd/2) * Y)^2 -
    (Y - (1 - sigma.sqd/2) * X[, i-1])^2))
      
    if (log(U[i]) < lg.alpha) {
      X[, i] <- Y
      count <- count + 1
      sum_sq_jumps <- sum_sq_jumps + sum((Y-X[, i-1])^2)
    } else {
      X[, i] <- X[, i-1]
    }
  }
  msjd <- sum_sq_jumps / (n-1)
  
  return(list(X=X, accepted=count, acceptance_rate = count/(n-1), msjd=msjd))
}
```

```{r, MALA optimal scaling for p and l}
p.vals <- c(1, 5, 20, 50, 100)
l.vals <- seq(0.1, 3, length.out=30)

results_mala <- data.frame()

for (p in p.vals) {
  for (l in l.vals) {
    res_mala <- MALA(n=5000, p=p, l=l)
    results_mala <- rbind(results_mala, data.frame(
      p=p,
      l=l,
      acc.rate=res_mala$acceptance_rate,
      msjd=res_mala$msjd
    ))
  }
}
```

```{r, plot of l against msjd for MALA, fig.width=8, fig.height=5}

ggplot(results_mala, aes(x=l, y=msjd, color=as.factor(p))) +
  geom_line() +
  geom_vline(xintercept = 1.7, linetype="dashed") +
  labs(title = "MALA: MSJD vs Scaling Parameter l",
       subtitle = "Dashed line represents theoretical optimum l as approximately 1.7",
       x = "Scaling (l)", y = "MSJD", color = "Dimension (p)") +
  theme_minimal()
```

```{r, plot of l against acc.rate for MALA, fig.width=8, fig.height=5}
ggplot(results_mala, aes(x=l, y=acc.rate, color=as.factor(p))) +
  geom_line() +
  geom_hline(yintercept = 0.574, linetype="dashed") + 
  labs(title = "MALA: Acceptance Rate vs Scaling Parameter l",
       subtitle = "Dashed line represents theoretical optimum rate = 0.574",
       x = "Scaling (l)", y = "Acceptance Rate", color = "Dimension (p)") +
  theme_minimal()
```
The first graph illustrates the efficiency of the chain via the MSJD. When $l$ is low, the MSJD is also low because the chain moves too slowly. When $l$ is large the MSJD is low because the chain gets 'stuck' due to high rejection rates. For MALA, there is a steep decline in MSJD after the optimum $l$ because the gradient term can push the chain so far into the tails that the drift term in the next step becomes really large leading to repeated rejections. This is also reflected in the second graph of accetance rate, with a sharp decline after the optimum $l \approx 1.7$.

For MALA we can see that the MSJD is almost 30 times larger than that of the RWMH in higher dimensions, here for $p=100$, which means that MALA is mixing a lot faster than RWMH. This demonstrates that MALA explores the space much more efficiently because the gradient information points the chain towards regions of high probability density, reducing the likelihood of proposing states in the tails. This demonstrates the duperiority of MALA in higher dimensions. A summary of the results are below. It is worth noting that the observed optimal scales vary slightly from theory due to the discrete grid of parameter values tested and noise.

```{r, summary of RWMH vs MALA}
results$Algorithm <- "RWMH"
results_mala$Algorithm <- "MALA"

all_data <- rbind(results, results_mala)

comparison_table <- all_data %>%
  group_by(Algorithm, p) %>%
  filter(msjd == max(msjd)) %>%
  dplyr::select(Algorithm, Dimension = p, Optimal_l = l, Max_MSJD = msjd, Acc_Rate = acc.rate) %>%
  arrange(Dimension, Algorithm)

print(comparison_table)

```

Below is an implementation of the systematic-scan Gibbs sampler for the Ising model.

```{r, Systematic-Scan Gibbs for Ising}
Systematic.Gibbs.Ising <- function(N=100, m=5, n=5, J, initial.state="random") {
  tr <- list()
  
  if (initial.state == "random") {
    x <- matrix(sample(c(-1, 1), m*n, replace=TRUE), nrow=m, ncol=n)
  }
  else if (initial.state == "all_plus") {
    x <- matrix(1, nrow=m, ncol=n)
  }
  else if (initial.state == "all_minus") {
    x <- matrix(-1, nrow=m, ncol=n)
  } else{
    stop("initial.state must be either 'random', 'all_plus' or 'all_minus'")
  }
  
  tr[[1]] <- x
  
  proj.trace <- numeric(N)
  
  neighbours <- function(rows, cols, r, c) {
    row.above <- if (r == 1) rows else r - 1
    row.below <- if (r == rows) 1 else r + 1
    col.left <- if (c == 1) cols else c - 1
    col.right <- if (c == cols) 1 else c + 1
    
    list(c(row.above, c), c(row.below, c), c(r, col.left), c(r, col.right))
  }
  
  for (t in 1:N) {
    for (i in 1:m) {
      for (j in 1:n) {
        
        ns <- neighbours(m, n, i, j)
        
        p1 <- 0
        for (k in 1:length(ns)) {
          p1 <- p1 + x[(ns[[k]])[1], (ns[[k]])[2]]
        }
        pp <- c(exp(-J*p1), exp(J*p1))
        pp <- pp / sum(pp)
        x[i,j] <- sample(c(-1,1), 1, prob=pp)
      }
    }
    tr[[t+1]] <- x
    proj.trace[t] <- sum(x==1)
  }
  return(list(trajectory=tr, trace=proj.trace))
}
```



```{r, results of Systematic-Scan Gibbs for Ising, fig.width=12, fig.height=8}
J.vals <- c(0.05, 0.5, 1)
initial.states <- c("random", "all_plus", "all_minus")
initial.labels <- c("Random Start", "All +1 Start", "All -1 Start")
ylim.range <- c(0, 5*5)
N <- 1000

par(mfrow=c(3,3), mar=c(4,4,3,1))

for (J in J.vals) {
  for (i in 1:length(initial.states)) {
    state.code <- initial.states[i]
    state.label <- initial.labels[i]
    
    chain <- Systematic.Gibbs.Ising(N=N, m=5, n=5, J=J, 
                                    initial.state=state.code)
    
    plot(chain$trace, type="l",  ylim=ylim.range,
         main=paste0("J=", J, " (", state.label, ")"),
         ylab="Count +1", xlab="Iteration")
  }
}

par(mfrow=c(1,1))
```

The implementation above considers three types of initialisation namely, all the states being +1, all the states being -1 and a random mixture of states. From the graphs above we can see how the different choices of initialisations and the strength of $J$ affects how the Markov chains behave.. When $J=0.5$, the choice of initialisation dictates which mode the chain starts and clearly reveals the multimodality of the target distribution. Defining a burn-in for $J=0.5$ is difficult because the chain flips between modes, for $J=0.05$ there is no burn-in required since the chain quickly mixes and for $J=1$ the chain is extremely 'sticky' meaning a burn-in would not help.


```{r, acf plots, fig.width=12, fig.height=8}
J.vals <- c(0.05, 0.5, 1)
initial.states <- c("random", "all_plus", "all_minus")
initial.labels <- c("Random Start", "All +1 Start", "All -1 Start")
ylim.range <- c(0, 5*5)
N <- 1000

par(mfrow=c(3,3), mar=c(4,4,3,1))

for (J in J.vals) {
  for (i in 1:length(initial.states)) {
    state.code <- initial.states[i]
    state.label <- initial.labels[i]
    
    chain <- Systematic.Gibbs.Ising(N=N, m=5, n=5, J=J, 
                                    initial.state=state.code)
    
    acf(chain$trace,
        lag.max=50,
        main = paste0("ACF: J=", J, " (", state.label, ")"),
        ylim = c(0,1),
        xlab = "Lag",
        ylab = "ACF"
      )
  }
}
par(mfrow=c(1,1))
```
From the ACF plots above we can see that for $J=0.05$ the ACF drops almost immediately to 0 indicating rapid mixing. For $J=0.5$, the chains initialised at "All +1" and "All -1" exibit strong autocorrelation that decays very slowly indicating that the chain is 'sticky'. Finally, for $J=0.5$, the ACF plots are mostly empty for "All +1" and "All -1" confirming that the chain is stuck in their initial states and have not converged to the stationary distribution at all.



The new algorithm is justified. The researcher defines a deterministic proposal for component $j$. Let $x^{(t)}$ be the current state and $x^{\prime}$ be the proposed state where the $j$-th state is flipped. The proposal distribution $q(x^{\prime}|x^{(t)})$ is

$$q(x^{\prime}|x^{(t)}) = 
\begin{cases} 
1 & \text{if } x_j^{\prime} = -x_j^{(t)} \;\; \text{and } x_{-j}^{\prime} = x_{-j}^{(t)} \\
0 & \text{otherwise}
\end{cases}$$

For the Metropolis-Hastings acceptance ratio, we consider

$$
\frac{q(x^{(t)}|x^{\prime})}{q(x^{\prime}|x^{(t)})}
$$
If we are at $x^{(t)}$, we propose $x^{\prime}$ with probability 1. If we are state $x^{\prime}$, the only move allowed by the proposal is to flip the state back, which returns us to $x^{(t)}$. Thus the probability of proposing $x^{(t)}$ given we are at state $x^{\prime}$ is also 1. Therefore, the proposal is symmetric:
$$
q(x^{\prime}|x^{(t)}) = q(x^{(t)}|x^{\prime}) = 1
$$
Since we have a symmetric proposal distribution the acceptance probability for the Metropolis-Hastings algorithm simplifies to
$$
\alpha(x^{(t)},x^{\prime}) = \min \left(1, \frac{f(x^{\prime})}{f(x^{(t)})}\right)
$$
Therefore, as the algorithm allows for the exploration of the state space, corrects for the proposal using the acceptance probability $\alpha$ and allows for rejection, we satisfy the detailed balance condition for the target distribution $f(x)$. Hence, the chain will converge to the stationary distribution defined by the Ising model $f(x)$.



```{r, Gibbs and MH for Ising}
Gibbs.MH.Ising <- function(N=100, m=5, n=5, J, initial.state="random") {
  tr <- list()
  
  if (initial.state == "random") {
    x <- matrix(sample(c(-1, 1), m*n, replace=TRUE), nrow=m, ncol=n)
  }
  else if (initial.state == "all_plus") {
    x <- matrix(1, nrow=m, ncol=n)
  }
  else if (initial.state == "all_minus") {
    x <- matrix(-1, nrow=m, ncol=n)
  } else{
    stop("initial.state must be either 'random', 'all_plus' or 'all_minus'")
  }
  
  tr[[1]] <- x
  
  proj.trace <- numeric(N)
  
  neighbours <- function(rows, cols, r, c) {
    row.above <- if (r == 1) rows else r - 1
    row.below <- if (r == rows) 1 else r + 1
    col.left <- if (c == 1) cols else c - 1
    col.right <- if (c == cols) 1 else c + 1
    
    list(c(row.above, c), c(row.below, c), c(r, col.left), c(r, col.right))
  }
  
  for (t in 1:N) {
    for (i in 1:m) {
      for (j in 1:n) {
        
        ns <- neighbours(m, n, i, j)
        
        p1 <- 0
        for (k in 1:length(ns)) {
          p1 <- p1 + x[(ns[[k]])[1], (ns[[k]])[2]]
        }
        
        ratio <- exp((-2) * J * x[i, j] * p1)
        
        if (runif(1) < ratio) {
          x[i, j] <- -x[i, j]
        }
        
      }
    }
    tr[[t+1]] <- x
    proj.trace[t] <- sum(x==1)
  }
  return(list(trajectory=tr, trace=proj.trace))
}
```

```{r, results for Gibbs MH Ising, fig.width=12, fig.height=8}
J.vals <- c(0.9)
initial.states <- c("random", "all_plus", "all_minus")
initial.labels <- c("Random Start", "All +1 Start", "All -1 Start")
ylim.range <- c(0, 5*5)
N <- 1000

par(mfrow=c(3,1), mar=c(4,4,3,1))

for (J in J.vals) {
  for (i in 1:length(initial.states)) {
    state.code <- initial.states[i]
    state.label <- initial.labels[i]
    
    chain <- Gibbs.MH.Ising(N=N, m=5, n=5, J=J, 
                                    initial.state=state.code)
    
    plot(chain$trace, type="l",  ylim=ylim.range,
         main=paste0("J=", J, " (", state.label, ")"),
         ylab="Count +1", xlab="Iteration")
  }
}

par(mfrow=c(1,1))
```
The performance of the Metropolis-Hastings algorithm is comparable to the Systematic-Scan Gibbs sample which is expected because both use single-state updates. The MH algorithm does not offer a significant improvement in mixing over the Gibbs sampler even though the the MH algorithm is computationally more efficient. The MH algorithm still suffers from the same path-dependence limitations as the Gibbs sampler. Neither algorithm is suitable for sampling from the Ising model at high $J$ without augmentation. 